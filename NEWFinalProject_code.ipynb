{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing USDA Food Data Central API for Nutritional Information\n",
    "\n",
    "This script interacts with the USDA Food Data Central API to search for food items and retrieve nutritional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDC ID: 770088, Description: OREO COOKIES\n",
      "\n",
      "Nutritional Information for OREO COOKIES:\n",
      "Carbohydrate, by difference: 73.0 g\n",
      "Total lipid (fat): 20.0 g\n",
      "Fiber, total dietary: 2.5 g\n",
      "Sodium, Na: 400.0 mg\n",
      "Total Sugars: 38.0 g\n",
      "Protein: 5.0 g\n",
      "Fatty acids, total saturated: 9.8 g\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "\n",
    "# Define your API key (it should be the actual key, not \"OAS3.0:\")\n",
    "api_key = 'M3175TAXsEcfdKkCOpNO9VcbwjQtFOkl9rUDAiaP'\n",
    "\n",
    "# Function to search for a food item and get its FDC ID\n",
    "def search_food(query):\n",
    "    #This is the data set for the USDA government\n",
    "    url = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
    "    # my parameters are query for the \n",
    "    params = {\n",
    "        \"query\": query, # search term enterd by user \n",
    "        \"api_key\": api_key, \n",
    "        \"pageSize\": 1  # Return only the top result\n",
    "    }\n",
    "    # using request.get() to send a get request to the API endpoint\n",
    "    response = requests.get(url, params=params)\n",
    "    #lines checks for the Https status and if it 200 then its okay\n",
    "    if response.status_code == 200:\n",
    "        # returns the json format to dictionary\n",
    "        data = response.json()\n",
    "        #checks for food items \n",
    "        if data['foods']:\n",
    "            # this will get the first result \n",
    "            food_item = data['foods'][0]\n",
    "            # return the food id and its description\n",
    "            return food_item['fdcId'], food_item['description']\n",
    "        else:\n",
    "           #No foods were found\n",
    "            print(\"No foods found.\")\n",
    "            return None, None\n",
    "    else:\n",
    "      # it will print out the error responce code\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to retrieve detailed nutritional information using FDC ID\n",
    "def get_food_details(fdc_id):\n",
    "\n",
    "    url = f\"https://api.nal.usda.gov/fdc/v1/food/{fdc_id}\"\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"format\": \"full\"#All available details for food\n",
    "    }\n",
    "    # Returns the full detail information for the food \n",
    "    response = requests.get(url, params=params)\n",
    "    # Same thing as earlier \n",
    "    if response.status_code == 200:\n",
    "        food_details = response.json()\n",
    "        return food_details\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# whatever the user wants\n",
    "\n",
    "foodname = str(input(\"Please enter the name of the object you want the nutrition data for: \"))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    food_query = foodname\n",
    "    fdc_id, description = search_food(food_query)\n",
    "    \n",
    "    if fdc_id:\n",
    "        print(f\"FDC ID: {fdc_id}, Description: {description}\")\n",
    "        food_details = get_food_details(fdc_id)\n",
    "        \n",
    "        # Print out some of the nutritional information\n",
    "        if food_details:\n",
    "            print(f\"\\nNutritional Information for {description}:\")\n",
    "            for nutrient in food_details.get('foodNutrients', []):\n",
    "                print(f\"{nutrient['nutrient']['name']}: {nutrient['amount']} {nutrient['nutrient']['unitName']}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve food details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Video Frames in Smaller Batches\n",
    "\n",
    "1. Process multiple videos from the input folder.\n",
    "2. Create a new numbered subfolder within the output folder for each video.\n",
    "3. Name each subfolder with a sequential number followed by _vid.\n",
    "4. Store the extracted frames as JPG files within their respective subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_to_frames_in_smaller_batches(video_folder, output_folder, desired_fps):\n",
    "    print(f\"Processing videos in {video_folder} to {output_folder} with {desired_fps} FPS\")\n",
    "    # List all video files in the selected folder\n",
    "    video_files = [f for f in os.listdir(video_folder) if os.path.isfile(os.path.join(video_folder, f)) and f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the selected folder.\")\n",
    "        return\n",
    "\n",
    "    for idx, video_file in enumerate(video_files, start=1):\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "\n",
    "        # Create a new subfolder in the output folder named <number>_vid\n",
    "        video_output_folder = os.path.join(output_folder, f'{idx}_vid')\n",
    "        os.makedirs(video_output_folder, exist_ok=True)\n",
    "\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        if original_fps <= 0:\n",
    "            print(f\"Warning: Unable to retrieve FPS for video {video_file}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the frame interval to match the desired FPS\n",
    "        frame_interval = int(round(original_fps / desired_fps))\n",
    "\n",
    "        if frame_interval <= 0:\n",
    "            frame_interval = 1  # Ensure at least every frame is processed\n",
    "\n",
    "        frame_count = 0\n",
    "        saved_frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Save frames at the specified interval into the specific subfolder\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frame_filename = os.path.join(video_output_folder, f'frame_{saved_frame_count:05d}.jpg')\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "                saved_frame_count += 1\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Extracted frames from '{video_file}' into the folder '{video_output_folder}'.\")\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video file selection\n",
    "\n",
    "This code cell creates a graphical user interface (GUI) using Python's `tkinter` library to assist users in processing video files. The GUI guides the user through the following steps:\n",
    "\n",
    "1. **Greeting the User**: Asks if they would like to process videos.\n",
    "2. **Selecting a Video Folder**: Opens a dialog for the user to choose a folder containing video files.\n",
    "3. **Handling Missing Files**: Provides help if the user cannot find the desired video folder.\n",
    "4. **Prompting for Frames Per Second (FPS)**: Asks the user to input the desired frames per second (FPS) for the video processing.\n",
    "5. **Selecting an Output Folder**: Prompts the user to choose or create a folder where the processed frames will be saved.\n",
    "6. **Handling Default Output Folder**: If the user does not select an output folder, the program creates a default folder named `output_file` in the current working directory.\n",
    "7. **Handling Duplicate Output Folders**: If a folder named `output_file` already exists, the program notifies the user and terminates to avoid conflicts.\n",
    "8. **Processing the Videos**: Processes the videos by extracting frames at the specified FPS and saves them in the output folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos in /Users/taneshafuller/Documents/ProjectRene/segment-anything-2/Video_folder to /Users/taneshafuller/Documents/ProjectRene/segment-anything-2/output with 55 FPS\n",
      "Extracted frames from 'IMG_5292.mov' into the folder '/Users/taneshafuller/Documents/ProjectRene/segment-anything-2/output/1_vid'.\n",
      "Extracted frames from 'IMG_5291.mov' into the folder '/Users/taneshafuller/Documents/ProjectRene/segment-anything-2/output/2_vid'.\n",
      "Extracted frames from 'IMG_5284.mov' into the folder '/Users/taneshafuller/Documents/ProjectRene/segment-anything-2/output/3_vid'.\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, simpledialog\n",
    "\n",
    "\n",
    "# Function to handle when the user can't find their file (to be implemented)\n",
    "def handle_cant_find_file():\n",
    "    # Placeholder for the file search assistance logic\n",
    "    messagebox.showinfo(\"Help\", \"Please check your folder structure and try again.\")\n",
    "\n",
    "\n",
    "\n",
    "def greet_and_prompt():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "\n",
    "    # Greet the user\n",
    "    user_response = messagebox.askquestion(\"Welcome\", \"Hello! Would you like to process videos?\")\n",
    "\n",
    "    if user_response == 'yes':\n",
    "        while True:\n",
    "            # Ask the user to select the folder containing video files\n",
    "            video_folder = filedialog.askdirectory(title=\"Select the Folder Containing Videos\")\n",
    "\n",
    "            if video_folder:\n",
    "                break  # Break the loop if a folder is selected\n",
    "            else:\n",
    "                # Option for the user if they can't find the folder\n",
    "                cant_find_response = messagebox.askquestion(\"Can't Find Folder\",\n",
    "                                                            \"Can't find your folder? Would you like some help?\",\n",
    "                                                            icon='warning')\n",
    "                if cant_find_response == 'yes':\n",
    "                    handle_cant_find_file()\n",
    "                else:\n",
    "                    messagebox.showinfo(\"Goodbye\", \"You chose not to proceed. Goodbye!\")\n",
    "                    root.update()  # Process any pending events\n",
    "                    root.quit()  # Exit the main loop\n",
    "                    root.destroy()  # Destroy the Tkinter root window\n",
    "                    return  # Exit the function and stop the script\n",
    "\n",
    "        # Prompt the user to select the output folder\n",
    "        select_output_response = messagebox.askquestion(\"Output Folder\", \"Would you like to select an output folder now?\")\n",
    "        \n",
    "        if select_output_response == 'yes':\n",
    "            while True:\n",
    "                output_folder = filedialog.askdirectory(title=\"Select Output Folder\")\n",
    "\n",
    "                if output_folder:\n",
    "                    messagebox.showinfo(\"Folder Selected\", f\"Output folder selected: {output_folder}\")\n",
    "                    break  # Proceed if a valid output folder is selected\n",
    "                else:\n",
    "                    # Notify the user to select a folder or quit the process\n",
    "                    retry_response = messagebox.askquestion(\"No Folder Selected\",\n",
    "                                                            \"You haven't selected an output folder. Would you like to try again?\",\n",
    "                                                            icon='warning')\n",
    "                    if retry_response == 'no':\n",
    "                        messagebox.showinfo(\"Goodbye\", \"You chose not to proceed. Goodbye!\")\n",
    "                        root.update()  # Process any pending events\n",
    "                        root.quit()  # Exit the main loop\n",
    "                        root.destroy()  # Destroy the Tkinter root window\n",
    "                        return  # Exit the function and stop the script\n",
    "        else:\n",
    "            messagebox.showinfo(\"Goodbye\", \"You chose not to select an output folder. Goodbye!\")\n",
    "            root.update()  # Process any pending events\n",
    "            root.quit()  # Exit the main loop\n",
    "            root.destroy()  # Destroy the Tkinter root window\n",
    "            return  # Exit the function and stop the script\n",
    "\n",
    "        # Ask the user to input the frames per second (FPS)\n",
    "        fps = simpledialog.askinteger(\"Frames Per Second\", \"Please enter the frames per second (FPS) value:\", minvalue=1)\n",
    "\n",
    "        if fps:\n",
    "            # Call the function with the gathered information\n",
    "            video_to_frames_in_smaller_batches(video_folder, output_folder, int(fps))\n",
    "            messagebox.showinfo(\"Process Complete\", f\"Videos processed with {fps} frames per second.\")\n",
    "        else:\n",
    "            messagebox.showinfo(\"No FPS Entered\", \"You didn't enter a valid FPS value. Goodbye!\")\n",
    "    \n",
    "    else:\n",
    "        messagebox.showinfo(\"Goodbye\", \"You chose not to proceed. Goodbye!\")\n",
    "    \n",
    "    root.update()  # Process any pending events\n",
    "    root.quit()  # Exit the main loop\n",
    "    root.destroy()  # Destroy the Tkinter root window\n",
    "\n",
    "\n",
    "# Call the function\n",
    "greet_and_prompt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video segmentation with SAM 2\n",
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating Images with Bounding Boxes for Object Detection\n",
    "\n",
    "This script helps annotate objects in images by drawing bounding boxes and saving them as training labels for models.\n",
    "If running locally using jupyter, first install `segment-anything-2` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything-2#installation) in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set env for SAM2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False\n",
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SAM2 Video Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/sam2_hiera_large.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sam2_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../checkpoints/sam2_hiera_large.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msam2_hiera_l.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m build_sam2_video_predictor(model_cfg, sam2_checkpoint, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sam2/build_sam.py:74\u001b[0m, in \u001b[0;36mbuild_sam2_video_predictor\u001b[0;34m(config_file, ckpt_path, device, mode, hydra_overrides_extra, apply_postprocessing, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(cfg)\n\u001b[1;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m _load_checkpoint(model, ckpt_path)\n\u001b[1;32m     75\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sam2/build_sam.py:121\u001b[0m, in \u001b[0;36m_load_checkpoint\u001b[0;34m(model, ckpt_path)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_checkpoint\u001b[39m(model, ckpt_path):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         sd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    122\u001b[0m         missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd)\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../checkpoints/sam2_hiera_large.pt'"
     ]
    }
   ],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
