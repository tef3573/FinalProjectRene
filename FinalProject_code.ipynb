{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing USDA Food Data Central API for Nutritional Information\n",
    "\n",
    "This script interacts with the USDA Food Data Central API to search for food items and retrieve nutritional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDC ID: 454004, Description: APPLE\n",
      "\n",
      "Nutritional Information for APPLE:\n",
      "Fatty acids, total saturated: 0.0 g\n",
      "Calcium, Ca: 0.0 mg\n",
      "Vitamin C, total ascorbic acid: 3.1 mg\n",
      "Cholesterol: 0.0 mg\n",
      "Iron, Fe: 0.23 mg\n",
      "Sodium, Na: 0.0 mg\n",
      "Vitamin A, IU: 65.0 IU\n",
      "Total Sugars: 10.39 g\n",
      "Energy: 52.0 kcal\n",
      "Potassium, K: 110.0 mg\n",
      "Protein: 0.0 g\n",
      "Fatty acids, total trans: 0.0 g\n",
      "Fiber, total dietary: 3.2 g\n",
      "Carbohydrate, by difference: 14.29 g\n",
      "Total lipid (fat): 0.65 g\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "\n",
    "# Define your API key (it should be the actual key, not \"OAS3.0:\")\n",
    "api_key = 'M3175TAXsEcfdKkCOpNO9VcbwjQtFOkl9rUDAiaP'\n",
    "\n",
    "# Function to search for a food item and get its FDC ID\n",
    "def search_food(query):\n",
    "    #This is the data set for the USDA government\n",
    "    url = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
    "    # my parameters are query for the \n",
    "    params = {\n",
    "        \"query\": query, # search term enterd by user \n",
    "        \"api_key\": api_key, \n",
    "        \"pageSize\": 1  # Return only the top result\n",
    "    }\n",
    "    # using request.get() to send a get request to the API endpoint\n",
    "    response = requests.get(url, params=params)\n",
    "    #lines checks for the Https status and if it 200 then its okay\n",
    "    if response.status_code == 200:\n",
    "        # returns the json format to dictionary\n",
    "        data = response.json()\n",
    "        #checks for food items \n",
    "        if data['foods']:\n",
    "            # this will get the first result \n",
    "            food_item = data['foods'][0]\n",
    "            # return the food id and its description\n",
    "            return food_item['fdcId'], food_item['description']\n",
    "        else:\n",
    "           #No foods were found\n",
    "            print(\"No foods found.\")\n",
    "            return None, None\n",
    "    else:\n",
    "      # it will print out the error responce code\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to retrieve detailed nutritional information using FDC ID\n",
    "def get_food_details(fdc_id):\n",
    "\n",
    "    url = f\"https://api.nal.usda.gov/fdc/v1/food/{fdc_id}\"\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"format\": \"full\"#All available details for food\n",
    "    }\n",
    "    # Returns the full detail information for the food \n",
    "    response = requests.get(url, params=params)\n",
    "    # Same thing as earlier \n",
    "    if response.status_code == 200:\n",
    "        food_details = response.json()\n",
    "        return food_details\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# whatever the user wants\n",
    "foodname = \"apple\"\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    food_query = foodname\n",
    "    fdc_id, description = search_food(food_query)\n",
    "    \n",
    "    if fdc_id:\n",
    "        print(f\"FDC ID: {fdc_id}, Description: {description}\")\n",
    "        food_details = get_food_details(fdc_id)\n",
    "        \n",
    "        # Print out some of the nutritional information\n",
    "        if food_details:\n",
    "            print(f\"\\nNutritional Information for {description}:\")\n",
    "            for nutrient in food_details.get('foodNutrients', []):\n",
    "                print(f\"{nutrient['nutrient']['name']}: {nutrient['amount']} {nutrient['nutrient']['unitName']}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve food details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Video Frames in Smaller Batches\n",
    "\n",
    "This script extracts frames from a video file and saves them in smaller batches within a designated folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 205 frames into 1 batches in the folder '/Users/taneshafuller/Documents/ProjectRene/FinalDataset/CutTuna2images/5900834-uhd_2160_4096_25fps'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def video_to_frames_in_smaller_batches(video_file, original_folder, batch_size=300):\n",
    "    # Create a new folder inside the original folder based on the video file name\n",
    "    video_filename = os.path.basename(video_file)\n",
    "    video_name, _ = os.path.splitext(video_filename)\n",
    "    output_folder = os.path.join(original_folder, video_name)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    batch_count = 0\n",
    "    batch_folder = os.path.join(output_folder, f'batch_{batch_count:03d}')\n",
    "    os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the frame to the batch folder\n",
    "        frame_filename = os.path.join(batch_folder, f'frame_{frame_count:05d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Create a new batch folder after every batch_size frames\n",
    "        if frame_count % batch_size == 0:\n",
    "            batch_count += 1\n",
    "            batch_folder = os.path.join(output_folder, f'batch_{batch_count:03d}')\n",
    "            os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames into {batch_count + 1} batches in the folder '{output_folder}'.\")\n",
    "\n",
    "# Example usage with a writable directory:\n",
    "video_file = \"/Users/taneshafuller/Desktop/FinalProductDataset/\"  # Replace with your video file path\n",
    "original_folder = os.path.expanduser(\"~/Documents/ProjectRene/FinalDataset/CutTuna2images\")  # Change to a writable location\n",
    "video_to_frames_in_smaller_batches(video_file, original_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video segmentation with SAM 2\n",
    "This notebook shows how to use SAM 2 for interactive segmentation in videos. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating Images with Bounding Boxes for Object Detection\n",
    "\n",
    "This script helps annotate objects in images by drawing bounding boxes and saving them as training labels for models.\n",
    "If running locally using jupyter, first install `segment-anything-2` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything-2#installation) in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set env for SAM2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False\n",
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "\n",
      "Support for MPS devices is preliminary. SAM 2 is trained with CUDA and might give numerically different outputs and sometimes degraded performance on MPS. See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SAM2 Video Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/sam2_hiera_large.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m sam2_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../checkpoints/sam2_hiera_large.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msam2_hiera_l.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m build_sam2_video_predictor(model_cfg, sam2_checkpoint, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sam2/build_sam.py:74\u001b[0m, in \u001b[0;36mbuild_sam2_video_predictor\u001b[0;34m(config_file, ckpt_path, device, mode, hydra_overrides_extra, apply_postprocessing, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m OmegaConf\u001b[38;5;241m.\u001b[39mresolve(cfg)\n\u001b[1;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mmodel, _recursive_\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m _load_checkpoint(model, ckpt_path)\n\u001b[1;32m     75\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sam2/build_sam.py:121\u001b[0m, in \u001b[0;36m_load_checkpoint\u001b[0;34m(model, ckpt_path)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_checkpoint\u001b[39m(model, ckpt_path):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckpt_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         sd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    122\u001b[0m         missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd)\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../checkpoints/sam2_hiera_large.pt'"
     ]
    }
   ],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
